{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3446bd1",
   "metadata": {},
   "source": [
    "**Instituto Tecnológico de Aeronáutica – ITA**\n",
    "\n",
    "**Visão Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Elcio Hideiti Shiguemori\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "**Orientações padrão:**\n",
    "\n",
    "Antes de você entregar o Lab, tenha certeza de que tudo está rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as células (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as células rodem sem erros, principalmente as de correção automática que apresentem os `assert`s.\n",
    "\n",
    "É muito importante que vocês não apaguem as células de resposta para preenchimento, isto é, as que contenham o `ESCREVA SEU CÓDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", além das células dos `assert`, pois elas contém metadados com o id da célula para os sistemas de correção automatizada e manual. O sistema de correção automatizada executa todo o código do notebook, adicionando testes extras nas células de teste. Não tem problema vocês criarem mais células, mas não apaguem as células de correção. Mantenham a solução dentro do espaço determinado, por organização. Se por acidente acontecer de apagarem alguma célula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se possível), pois não adianta recriar a célula porque perdeu o ID.\n",
    "\n",
    "Os Notebooks foram programados para serem compatíveis com o Google Colab, instalando as dependências necessárias automaticamente a baixando os datasets necessários a cada Lab. Os comandos que se inicial por ! (ponto de exclamação) são de bash e também podem ser executados no terminal linux, que justamente instalam as dependências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49bb8c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac376f4",
   "metadata": {},
   "source": [
    "# Lab de Processamento 3d\n",
    "\n",
    "Neste laboratório iremos revisar o modelo de câmera a fim de aplicar em um exemplo de perspectiva de n pontos, de calibração estéreo e também de retificação a fim de facilitar a correspondência estéreo. Por fim veremos um exemplo do mapa de disparidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ab79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install opencv-contrib-python==4.6.0.66 Pillow==7.1.2 matplotlib==3.2.2 scipy==1.7.3 gdown\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3638147",
   "metadata": {},
   "source": [
    "Se for executar local e quiser alterar o Path, pode alterar aqui sem problemas para a correção automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -d \"/content/calib_esq\" ] && gdown -O /content/calib_esq.zip 1vg2fnoLjcYAdF44HxwK41basZUj0xYMN && unzip /content/calib_esq.zip -d /content && rm /content/calib_esq.zip\n",
    "! [ ! -d \"/content/calib_dir\" ] && gdown -O /content/calib_dir.zip 1d0OeP9YCxx1sWjwDMvf0y6f35Lx3ELPN && unzip /content/calib_dir.zip -d /content && rm /content/calib_dir.zip\n",
    "! [ ! -d \"/content/tsukuba\" ] && gdown -O /content/tsukuba.zip 1Ghpx9_x8E26SzJP3X-Itt94QD3yVM8lT && unzip /content/tsukuba.zip -d /content && rm /content/tsukuba.zip\n",
    "root_path = Path(\"/content\")\n",
    "imgs_esq_path = root_path/\"calib_esq\"\n",
    "imgs_dir_path = root_path/\"calib_dir\"\n",
    "tsukuba_path = root_path/\"tsukuba\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994dd34",
   "metadata": {},
   "source": [
    "No Jupyter notebook, aperte Shift+TAB seguidas vezes (uma, duas ou três) para aparecer um tooltip com a assinatura de uma função. No Colab, para fazer isso com o TAB tem que primeiro desativa o `Automatically trigger code completions` no `Tools->Settings->Editor`, se não ele só vai aparecer automaticamente quando você começar a digitar a função, mas não com o TAB.\n",
    "\n",
    "![Jupyter Notebook Tooltip](https://camo.githubusercontent.com/88055882678674dd9c9e084ffa7d096ad9723d4ed6736e51f050a69d8fb307d0/687474703a2f2f672e7265636f726469742e636f2f4b7955513569754a64762e676966)\n",
    "\n",
    "Você também pode colocar um ponto de interrogação no final de uma função para abrir o help dela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37217ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a4f26",
   "metadata": {},
   "source": [
    "## Revisão Modelo da Câmera\n",
    "\n",
    "Essa parte inicial é apenas uma revisão básica com funções cuja solução é apenas uma linha. A intenção é exercitar e revisar alguns conceitos básicos, para que você consiga estimar a matriz intrínseca de uma câmera sem precisar de calibração com tabuleiro de xadrez (apenas com algumas especificações, como FoV+resolução ou distância focal+tamanho do sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c45f57",
   "metadata": {},
   "source": [
    "### Equivalência Lente Pinhole\n",
    "Nesta primeira etapa vamos relembrar algumas propriedades do modelo de câmera projetiva. O nosso modelo de projeção é retilinear, que é o mesmo obtido por uma câmera pinhole, no qual os raios de luz convergem (são filtrados) em um único ponto (centro de projeção) e terminam em um aparato (sensor), formando uma imagem invertida (nos dois eixos).\n",
    "Uma câmera de lente delgada (ou ainda um conjunto de lentes que seja equivalente, o que é o caso das câmeras de hoje em dia que são várias lentes) também é equivalente ao modelo pinhole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e019db",
   "metadata": {},
   "source": [
    "\n",
    "**ENTENDA:** Tenha cuidado com os termos de distância focal, que muita gente confunde, pois na prática são próximos numericamente, como veremos a diante. Nas equações de projeção, de calibração, de cálculo de FOV simplificado, o que nós estamos nos referindo quando falamos em **DISTÂNCIA FOCAL**, é o equivalente do modelo pinhole $f_{pinhole}$ da figura abaixo, isto é, a distância do centro de projeção até o plano de projeção, como ilustrado na figura abaixo.\n",
    "\n",
    "![Modelo de Câmera Pinhole](https://gam.dev/cm203/imgs/pinhole.svg)\n",
    "\n",
    "Já a lente (ou sistema de lentes) tem a sua própria distância focal $f_{lente}$ e a distância da lente até a imagem formada $d_i$ é função do plano de foco, isto é, a distância do objeto $d_i$ que está em foco até a lente. Conforme figura abaixo. Essa relação é regida pela equação das lentes delgadas (que é apenas semelhança de triangulos), a ser aplicada no exercício adiante.\n",
    "\n",
    "Assim perceba que a equivalência é entre $d_i$ e $f_{pinhole}$ e não ao $f_{lente}$. Quando o objeto estiver longe ($d_o >> f$), inevitavelmente $d_i$ vai se aproximar de $f_{lente}$.\n",
    "\n",
    "![Modelo de Câmera com Lente Equivalente](https://gam.dev/cm203/imgs/lens.svg)\n",
    "\n",
    "Assim, vamos calcular essa equivalência, implemente a função abaixo: (0,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9661f51",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb75d479eeb89dfaee63e342b2bb18c",
     "grade": false,
     "grade_id": "equacao_pinhole_f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def distancia_focal_pinhole_equivalente(f_lente, d_o):\n",
    "    \"\"\"\n",
    "    Implemente a equação que calcula a distância focal equivalente a uma câmera pinhole (d_i)\n",
    "    com a restrição de 0 < lens_focal_lenght < focus_plane_distance\n",
    "    :param f_lente: distância focal da lente, em metros\n",
    "    :param d_o: distância da lente ao plano do objeto em foco, em metros\n",
    "    Retorna a distância focal equivalênte a uma câmera pinhole, em metros\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return f_pinhole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2708b",
   "metadata": {},
   "source": [
    "Lembre da equação das lentes:\n",
    "\n",
    "![Equação das Lentes Delgadas](https://gam.dev/cm203/imgs/lentes_delgadas.svg)\n",
    "\n",
    "Verifique se a sua implementação está correta executando a célula abaixo (0,5 pontos).\n",
    "\n",
    "Perceba no caso de teste que para uma distância focal de 8 mm e um plano de foco a 10 m, $f_{pinhole}$ é praticamente igual a $f_{lente}$, uma diferença menor que 0.08%.\n",
    "\n",
    "Essas células com `assert` serão corrigidas automaticamente pelo corretor, nós deixamos pelo menos um caso de teste visível para vocês verificarem que está certo, mas claro que temos outros casos de teste, então nem adianta criar uma função que retorna uma constante.\n",
    "\n",
    "Essa célula, juntamente com a célula assim tem um ID de metadado que é usado para a correção automática, então se por acaso você apagá-la, só pode desfazer pelo `Edit->Undo`, não adianta criar uma nova célula e copiar o código porque perdeu o ID. Se o Undo não funcionar, recomendo baixar o notebook de novo e cópiar as suas soluções para o notebook novo com a célula original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0448d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96645b64fa40f396f1514cab3d97ec59",
     "grade": true,
     "grade_id": "corrige_equacao_pinhole_f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(distancia_focal_pinhole_equivalente(0.008,  10) - 0.00800640) < 1e-6\n",
    "# OBS: implemente uma função np friendly (que eu consiga passar um vetor de numpy)\n",
    "assert abs(distancia_focal_pinhole_equivalente(np.array([0.008]),  np.array([10.])) - 0.00800640) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbcc20",
   "metadata": {},
   "source": [
    "### Campo de Visão FoV\n",
    "\n",
    "Agora vamos calcular o FOV (em graus) de uma câmera pinhole com base na largura do sensor e a distância do sensor ao centro de projeção, considerando que o centro de projeção coincida com o centro da imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0ed7a",
   "metadata": {},
   "source": [
    "![Campo de Visão (Field of View) FoV](https://gam.dev/cm203/imgs/fov.svg)\n",
    "\n",
    "Implemente a fução abaixo que calcula o FoV (0,5 pontos)\n",
    "\n",
    "<details><summary><b>Dica</b></summary>\n",
    "<p>\n",
    "Usar a função `np.arctan` e a constante `np.pi`.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c54c6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "727a3aebd357ad865252b6c0430edb43",
     "grade": false,
     "grade_id": "equacao_fov",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calcula_fov(f_pinhole, largura_sensor):\n",
    "    \"\"\"\n",
    "    Calcula o campo de visão em graus de uma câmera pinhole\n",
    "    :param f_pinhole: distância focal da câmera pinhole, em metros\n",
    "    :param largura_sensor: largura do sensor, em metros\n",
    "    Retorna o campo de visão FoV em graus\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return FoV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f287c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e88855f39428d019518d01f06c50a1fb",
     "grade": true,
     "grade_id": "corrige_equacao_fov",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(calcula_fov(1,  2) - 90) < 1e-6\n",
    "# OBS: implemente uma função np friendly (que eu consiga passar um vetor de numpy)\n",
    "assert abs(calcula_fov(np.array([1.]),  np.array([2.]))[0] - 90) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219ec11",
   "metadata": {},
   "source": [
    "Agora vamos fazer o mesmo cálculo, mas agora considerando um modelo de lente que esteja focada a uma distância certa distância ($d_o$), isto é a distância ao plano em foco. (0,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050980ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e8ca23cbbd78350fa7808b253c935de",
     "grade": false,
     "grade_id": "equacao_fov_lente",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calcula_fov_lente(f_lente, largura_sensor, distancia_plano_em_foco):\n",
    "    \"\"\"\n",
    "    Calcula o campo de visão em graus de uma câmera de lente delgada\n",
    "    :param f_lente: distância focal da lente, em metros\n",
    "    :param largura_sensor: largura do sensor, em metros\n",
    "    :param distancia_plano_em_foco: distância da lente ao plano do objeto em foco, em metros\n",
    "    Retorna o campo de visão FoV em graus\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return FoV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782d4f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c93030ea81bbd73896d40fb0342dc7a",
     "grade": true,
     "grade_id": "corrige_equacao_fov_lente",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(calcula_fov_lente(0.001,  0.002, 100) - 89.999427) < 1e-6\n",
    "# OBS: implemente uma função np friendly (que eu consiga passar um vetor de numpy)\n",
    "assert abs(calcula_fov_lente(np.array([0.001]),  np.array([0.002]), np.array([100.]))[0] - 89.999427) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334cd05b",
   "metadata": {},
   "source": [
    "Perceba que o FoV varia bem pouco em função da distância do plano em foco. A variação é maior para objetos que estão mais próximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60daa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.07, 5, 99), calcula_fov_lente(np.array([0.001]*99), np.array([0.002]*99), np.linspace(0.07, 5,99)))\n",
    "plt.xlabel('Plano em foco (m)')\n",
    "plt.ylabel('Plano em foco (graus)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65258f3c",
   "metadata": {},
   "source": [
    "Agora vamos partir para um caso em que você pode usar na prática, quando o fabricante de uma câmera nos dá o campo de visão em graus e nós sabemos a largura em pixels da imagem que ela forma, vamos calcular qual a distância focal equivalente pinhole em pixels que ela nos dá. É esse $f_{pinhole}$ que usamos na matriz intríseca do formato do OpenCV. Nessa caso eu falo em largura e FoV horizontal mas também é a mesma coisa para a altura e FoV vertical. (0,5 pontos)\n",
    "\n",
    "\n",
    "<details><summary><b>Dica</b></summary>\n",
    "<p>\n",
    "Use `np.tan`\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd260d62",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2aab7cba4852e4f87439b42a9a903a6",
     "grade": false,
     "grade_id": "equacao_dist_focal",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calcula_dist_focal_pixels(FoV, largura_pixels):\n",
    "    \"\"\"\n",
    "    Calcula a distância focal em pixels de uma câmera pinhole, 0 < FoV < 180\n",
    "    :param FoV: Campo de Visão da câmera em graus\n",
    "    :param largura_pixels: Largura do sensor (da imagem formada) em pixels\n",
    "    Retorna a distância focal equivalente pinhole em pixels.\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return f_pinhole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee249da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f69c01d245d92405261164f6f00e6f24",
     "grade": true,
     "grade_id": "corrige_equacao_dist_focal",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(calcula_dist_focal_pixels(90, 1920) - 960) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdece06",
   "metadata": {},
   "source": [
    "No exercício acima, a distância focal está em pixels, mas também se quisermos converter em metros, teríamos que saber o tamamho do pixel (pixel size), ou de forma equivalente, a largura efetiva do sensor. Assim, seria apenas fazer uma conversão simples de unidades de pixels para metros.\n",
    "\n",
    "Espero que nessa etapa você tenha entendido a relação de equivalência entre a projeção retilinear (dada por uma câmera pinhole) e uma câmera com um conjunto de lentes delgadas, e principalmente que a distância focal equivalente pinhole dessas câmeras varia de acordo com o plano de foco (onde a imagem está focada), o que influencia até no FoV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3321d4e",
   "metadata": {},
   "source": [
    "### Coordenadas Homogêneas e Geometria Projetiva\n",
    "\n",
    "Vamos agora para o modelo projetivo: \n",
    "\n",
    "Para facilitar a matemática, isto é, não precisar inverter imagem nem se preocupar com coordenadas negativas em z, vamos colocar o plano de projeção à frente do centro de projeção.\n",
    "\n",
    "Perceba que o resultado é equivalente: a imagem projetada possui exatamente as mesmas dimensões, e a distância focal se mantém a mesma em módulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d52ba",
   "metadata": {},
   "source": [
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam.svg)\n",
    "\n",
    "Observe agora o nosso sistema de coordenadas da câmera, no qual a nossa origem está no Centro de Projeção. O nosso plano de projeção é ortogonal ao eixo Z, que aponta para fora da câmera.\n",
    "\n",
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam_proj.svg)\n",
    "\n",
    "Um ponto no mundo real $(X, Y, Z)$ medido em metros é projetado no plano da projeção por uma simples semelhança de triângulo, sabendo que o plano de projeção está afastado a uma distância $f_{metros}$ da origem (em metros, distância focal pinhole equivalente), em metros: $(X_p, Y_p, Z_p) = (X\\frac{f_{metros}}{Z}, Y\\frac{f_{metros}}{Zf_{metros}}, f)$ ainda em metros. OBS: estou chamando o $f_{pinhole}$ de $f_{metros}$ para dar ênfase nas unidades para que você não se confunda, é claro que metros poderia ser qualquer outra unidade medida, só quero diferenciar de pixels.\n",
    "\n",
    "\n",
    "![Câmera Pinhole Plano de projeção Frontal](https://gam.dev/cm203/imgs/pinhole_cam_proj_ext.svg)\n",
    "\n",
    "\n",
    "Se não fosse esse $Z$ dividindo, seria uma transformação linear e poderíamos escrever em forma de matriz. Agora vem o conceito de coordenadas homogêneas ao resgate! Basta acrescentar uma nova dimensão que vai servir como o *divisor* da normalização. Assim podemos expressar todas essas operações de divisão e até de soma como uma única multiplicação matricial.\n",
    "\n",
    "Para transformar um ponto N-dimensional cartesiano para seu equivalente homogêneo, basta acrescentar uma nova dimensão com valor unitário. E no inverso, basta dividir por esse valor, com cuidado para pontos no infinito (divisão por zero). Assim o ponto 2d cartesiano $(X, Y)$ fica $(X, Y, 1)$ no homogêneo. O ponto $(X, Y, Z, W)$ homogêneo fica $(X/W, Y/W, Z/W)$ no cartesiano. Veja que há infinitos pontos do homogeneo que mapeiam no mesmo do cartesiano.\n",
    "\n",
    "Assim nos exercícios abaixo (0,5 pontos cada), implemente a transformação de coordenada cartesiana para homogênea e virse-versa. Não se preocupe com os pontos no infinito.\n",
    "\n",
    "Matematicamente é trivial, mas deixei esses dois exercícios para vocês ficarem espertos com o numpy, a indexação, concatenação de vetores, e formato (shape). Principalmente o shape dos vetores que muita gente confunde e faz perder o shape quando indexa o vetor coluna.\n",
    "\n",
    "<details><summary><b>Dica</b></summary>\n",
    "<p>\n",
    "Use `np.vstack` (que recebe uma lista de vetores coluna), na hora de indexar use o operador `:` em todos os índices, use o índice `-1` para selecionar o último elemento.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6916017",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a36206e4aeea14be45175c86a9c08b67",
     "grade": false,
     "grade_id": "coord_carte_homoge",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cartesiano_para_homogeneo(vetor):\n",
    "    \"\"\"\n",
    "    Transforma de coordenadas cartesianas para homogêneas\n",
    "    :param vetor: Vetor coluna numpy em coordenada cartesiana\n",
    "    Retorna um vetor coluna numpy em coordenada homogênea\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return vetor_coord_homogenea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad7058",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cdd21bb17b05ad4f3e3b646da9bb19b",
     "grade": true,
     "grade_id": "testa_coord_carte_homoge",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.linalg.norm(cartesiano_para_homogeneo(np.array([[2],\n",
    "                                                          [4]])) - np.array([[2],\n",
    "                                                                             [4],\n",
    "                                                                             [1]])) < 1e-6\n",
    "assert np.linalg.norm(cartesiano_para_homogeneo(np.array([[19],\n",
    "                                                          [19],\n",
    "                                                          [19]])) - np.array([[19],\n",
    "                                                                              [19],\n",
    "                                                                              [19],\n",
    "                                                                              [1 ]])) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c09e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b745a9188beff38859fda6b2e2e9982",
     "grade": false,
     "grade_id": "coord_homoge_carte",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def homogeneo_para_cartesiano(vetor_coord_homogenea):\n",
    "    \"\"\"\n",
    "    Transforma de coordenadas homogêneas para cartesiana, sem se preocupar com ponto no infinito.\n",
    "    :param vetor_coord_homogenea: Vetor coluna numpy em coordenada homogênea\n",
    "    Retorna um vetor coluna numpy em coordenada cartesiana\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254c6a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f03c3e0a608172e20d99277c68e65d",
     "grade": true,
     "grade_id": "testa_coord_homoge_carte",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.linalg.norm(homogeneo_para_cartesiano(np.array([[4],\n",
    "                                                          [8],\n",
    "                                                          [2]])) - np.array([[2],\n",
    "                                                                             [4]])) < 1e-6\n",
    "assert np.linalg.norm(homogeneo_para_cartesiano(np.array([[19],\n",
    "                                                          [19],\n",
    "                                                          [19],\n",
    "                                                          [19]])) - np.array([[1],\n",
    "                                                                              [1],\n",
    "                                                                              [1]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79185af4",
   "metadata": {},
   "source": [
    "Mas nós queremos saber não onde está o ponto no sensor em metros, mas sim onde está o ponto na imagem em pixels, e lá vamos nós com uma simples regra de três (transformação linear):\n",
    "\n",
    "Primeiro temos que saber onde que o ponto $(0, 0, f_{metros})$ do plano de projeção está na imagem em pixels, que é o $(c_x, c_y)$. Depois, temos que saber a relação entre pixels e metros para transformar a distância focal de metros para pixels, isto é, o que chamamos de tamanho do pixel $\\mu$.\n",
    "\n",
    "Assim o nosso ponto 3d $(X_p, Y_p, Z_p)$ medido em metros vira simplesmente um ponto 2d $\\left(c_x + X_p\\mu, c_y + Y_p\\mu\\right)$ medido agora em pixels, na imagem. Substituindo, temos $\\left(c_x + X\\frac{f_{metros}}{Z}\\mu, c_y + Y\\frac{f_{metros}}{Z}\\mu\\right)$\n",
    "\n",
    "Perceba que o termo $f_{metros} \\mu$ é justamente a conversão da distância focal em metros para pixels, que vamos chamar agora simplesmente de $f$ que a convenção que o OpenCV usa, para a distância focal equivalente pinhole medida em pixels.\n",
    "\n",
    "Assim, temos o mapeamento de um ponto 3d $(X, Y, Z)$ em metros para $\\left(c_x + X\\frac{f}{Z}, c_y + Y\\frac{f}{Z}\\right)$, sendo os valores de $c_x$, $c_y$ e $f$ medidos em pixels.\n",
    "\n",
    "Em vez só de pesquisar a matriz de transformação projetiva na internet ou na aula direto, tente pensar um pouco. Lembre que a saída é em coordenada homogênea, então não se preocupe com a divisão pelo $Z$. (0,5 pontos)\n",
    "\n",
    "<details><summary><b>Dica de Numpy</b></summary>\n",
    "<p>\n",
    "Use operador `@` do numpy para multiplicação matricial `A @ B` equivale a `np.dot(A, B)`. A transformação de projeção é simplesmente uma multiplicação matricial da matriz de transformação pela coordenada, o que resulta em uma coordenada projetada em formato homogêneo.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c611651",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c33fb247daee04a738ebe20a45c6ee6d",
     "grade": false,
     "grade_id": "transf_proj",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def transformacao_projetiva_3d(ponto3d, c_x, c_y, f):\n",
    "    \"\"\"\n",
    "    Projeta um ponto no espaço para um ponto no plano em coordenadas homogêneas\n",
    "    :param ponto3d: ponto 3d em metros no formato de vetor coluna numpy\n",
    "    :param c_x: coordenadas no eixo x do ponto principal, em pixels\n",
    "    :param c_y: coordenadas no eixo y do ponto principal, em pixels\n",
    "    :param f: distância focal em pixels\n",
    "    Retorna um vetor coluna homogêneo referente à projeção e a matriz de transformação dessa projeção\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return ponto_projetado_2d_homogeneo, matriz_transformacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ad572",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "402f5d0200217cec60e1da7edcf597f4",
     "grade": true,
     "grade_id": "test_transf_proj",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ponto, mtx = transformacao_projetiva_3d(np.array([[1],[1],[1]]), 1, 1, 1) # Tudo 1 para não espiar :)\n",
    "assert np.linalg.norm(mtx/mtx[-1,-1] - np.array([[1, 0, 1], [0, 1, 1], [0, 0, 1]])) < 1e-6\n",
    "assert np.linalg.norm(homogeneo_para_cartesiano(ponto) - np.array([[2], [2]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3e226",
   "metadata": {},
   "source": [
    "Além do mapeamento anterior que leva a $\\left(c_x + X\\frac{f}{Z}, c_y + Y\\frac{f}{Z}\\right)$, pode-se considerar também uma distância focal diferente entre os eixos $x$ e $y$ da imagem, como se houvesse um achatamento em alguma dimensão (seja por uma lente cilíndrica, ou sensels (sensor de pixels) que não seja quadrados, ou redimensionamento). Dá-se o nome de *aspect ratio* $f_y = a f_x$ a esse quociente $a = \\frac{f_y}{f_x}$ (dependendo da biblioteca pode ser o recíproco).\n",
    "\n",
    "Outro termo, encontrado com menor frequência, é o *skew* da imagem, que modeleraria o caso das linhas dos sensores estiverem desalinhadas, formando um paralelogramo em vez de um retângulo, ou ainda para corrigir o fenômeno que ocorre em nas câmeras de *rolling shutter* quando a região de imagem está sempre se movendo a velocidade linear constante com relação à câmera. O OpenCV não apresenta esse parâmetro em seu modelo de calibração.\n",
    "\n",
    "Assim o modelo completo fica: $\\left(c_x + \\frac{X f_x + Y skew}{Z}, c_y + Y\\frac{f_y}{Z}\\right)$. Reescreva a equação acima de forma a isolar a matriz de projeção, retornando apenas a matriz intríseca da câmera que é exatamente a matriz de projeção com o cuidado de que o último termo é unitário. (0,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8897588",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9840a18fbe1057d8e34b081e964d9be2",
     "grade": false,
     "grade_id": "mtx",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forma_matriz_intriseca(c_x, c_y, f_x, f_y, skew):\n",
    "    \"\"\"\n",
    "    Forma a Matriz intrínseca (mtx) de uma câmera definida pelos parâmetros c_x, c_y, f_x, f_y, skew\n",
    "    Atente ao fato de que ela multiplicada por um vetor coluna de um ponto no mundo é o ponto homogêno na imagem\n",
    "    mtx @ np.array([[X],[Y],[Z]]) = np.array([[X*f_x+skew*Y+c_x*Z],[Y*f_y+c_y*Z],[Z]])\n",
    "    :param c_x: coordenadas no eixo x do ponto principal, em pixels\n",
    "    :param c_y: coordenadas no eixo y do ponto principal, em pixels\n",
    "    :param f_x: distância focal em pixels no eixo x\n",
    "    :param f_y: distância focal em pixels no eixo y\n",
    "    :param skew: coeficiente de cisalhamento, em pixels\n",
    "    Retorna mtx, atente que nesse caso mtx[2, 2] é necessariamente igual a 1 (normalizado).\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return matriz_intrinseca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720dadf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e496e3bd0f3e7e9a9612fc23355e22dd",
     "grade": true,
     "grade_id": "test_mtx",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mtx = forma_matriz_intriseca(1, 1, 1, 1, 1) # Tudo 1 para não espiar :) mas temos mais testes, cuida\n",
    "assert np.linalg.norm(mtx - np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346d359",
   "metadata": {},
   "source": [
    "## Perspectiva de N Pontos\n",
    "\n",
    "Vamos fazer uma aplicação direta da Perspectiva de N Pontos e refletir sobre suas limitações. A Perspectiva de N Pontos encontra a rotação e translação que minimizam o erro de reprojeção, dados os parâmetros intrísecos da câmera (já conhecidos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a97619",
   "metadata": {},
   "source": [
    "Encontre a pose da carga externa. Use a função `cv2.solvePnP`. \n",
    "Utilize também a flag `cv2.SOLVEPNP_EPNP`, pois o default `cv2.SOLVEPNP_ITERATIVE`, se não passsar uma estimativa inicial para o `[R, T]`, ele resolve um sistema linear (DLT) para fazer essa estimativa, e precisa de mais pontos.\n",
    "(1 ponto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3249d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cda7085a222e221cbf50df3e25aac7dd",
     "grade": false,
     "grade_id": "pnp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def encontra_pose_pnp(verts3d, verts2d, parametros_camera):\n",
    "    \"\"\"\n",
    "    Determina a pose de um objeto no espaço, dado sua geometria, os pontos correspondentes e a câmera calibrada\n",
    "    :param verts3d: as coordenadas dos pontos 3d de um objeto em seu referencial próprio, em metros\n",
    "    :param verts2d: as posições encontradas desses pontos na imagem, em pixels\n",
    "    :param parametros_camera: tupla com a matrix intríseca e coeficientes de distorção\n",
    "    Retorna o erro de reprojeção e os vetores de rotação (notação de Rodrigues, em radianos) e o de translação,\n",
    "    que define a pose do objeto com relação à câmera\n",
    "    \"\"\"\n",
    "    matrix_intrinseca, coefs_distorcao = parametros_camera\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return erro_reprojecao, rvec, tvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d0fa5",
   "metadata": {},
   "source": [
    "Veja que no teste abaixo parte-se de uma geometria e pose aleatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f792a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a06d7a8b7014befd43220de22f20d68",
     "grade": true,
     "grade_id": "testa_pnp",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "verts3d = np.array([[-1,-1,0],[-1,1,0],[1,1,0],[1,-1,0],[0,0,2]], dtype=np.float32)\n",
    "mtx = np.array([[1000,0,1000],[0,1000,500],[0,0,1]], dtype=np.float32)\n",
    "dist = np.zeros((5,), dtype=np.float32)\n",
    "rvec = np.array([[0.3], [-0.2], [0.6]])\n",
    "tvec = np.array([[0.1], [-0.2], [7]])\n",
    "verts2d, _ = cv2.projectPoints(verts3d, rvec, tvec, mtx, dist)\n",
    "erro_reprojecao, rvec_rec, tvec_rec = encontra_pose_pnp(verts3d, verts2d, (mtx, dist))\n",
    "assert np.linalg.norm(R.from_rotvec(rvec.ravel()).as_matrix() - R.from_rotvec(rvec_rec.ravel()).as_matrix()) < 0.01\n",
    "assert np.linalg.norm(tvec - tvec_rec) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcc76f",
   "metadata": {},
   "source": [
    "Vamos visualizar a projeção desses pontos no espaço. Fique a vontade para alterar os valores de rvec e tvec e a matriz intrínseca.\n",
    "\n",
    "Aqui estamos apenas ligando os pontos projetados com linhas verdes entre si, para tentar dar a noção do objeto no espaço (já está virando computação gráfica, o Doppelgänger da visão computacional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sintetica = np.zeros((1000, 2000, 3), dtype=np.uint8)\n",
    "f = 1000 * 1\n",
    "d = 4 * 1\n",
    "mtx = np.array([[f,0,1000],[0,f,500],[0,0,1]], dtype=np.float32)\n",
    "rvec = np.array([[0.7], [0.3], [0]], dtype=np.float32)\n",
    "tvec = np.array([[0.1], [0.8], [d]])\n",
    "pontos_projetados, _ = cv2.projectPoints(verts3d, rvec, tvec, mtx, dist)\n",
    "for i, p1 in enumerate(pontos_projetados):\n",
    "    if i + 2 < len(pontos_projetados):\n",
    "        p2 = pontos_projetados[i+1].astype(np.uint).ravel()\n",
    "    else:\n",
    "        p2 = pontos_projetados[0].astype(np.uint).ravel()\n",
    "    cv2.line(img_sintetica, p1.astype(np.uint).ravel(), p2, (0, 255, 0))\n",
    "    cv2.line(img_sintetica, p1.astype(np.uint).ravel(), pontos_projetados[-1].astype(np.uint).ravel(), (0, 255, 0))\n",
    "    cv2.circle(img_sintetica, p1.astype(np.uint).ravel(), 4, (0, 255, 0))\n",
    "cv2.drawFrameAxes(img_sintetica, mtx, dist, rvec, tvec, 1)\n",
    "\n",
    "PIL.Image.fromarray(cv2.cvtColor(img_sintetica, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd811953",
   "metadata": {},
   "source": [
    "[![Dolly Zoom](https://upload.wikimedia.org/wikipedia/commons/c/c7/Contra-zoom_aka_dolly_zoom_animation.gif)](https://upload.wikimedia.org/wikipedia/commons/transcoded/5/5f/DollyZoomTest.ogv/DollyZoomTest.ogv.480p.vp9.webm \"Dolly Zoom\")\n",
    "\n",
    "Entenda alguma das limitações que a Perspectiva de N Pontos tem. Principalmente devido ao fato de ser uma solução monocular.\n",
    "Descreva o que acontece com a exatidão da solução à medida que a distância focal da lente aumenta, considerando o tamanho da imagem constante e o tamanho do objeto focalizado também constante. A exatidão da solução em metros aumenta, diminui ou permanece constante? Para todos os eixos ou o comportamente é diferente para algum eixo específico X, Y ou Z? Explique.\n",
    "Escreva sua resposta na célula abaixo. (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d145284",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72868f9abfcea34ccc55bf30937efde9",
     "grade": true,
     "grade_id": "discute_pnp",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**QUESTÃO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (pode apagar comentário, mas não apague esta célula para não perder o ID)\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8334c5",
   "metadata": {},
   "source": [
    "## Calibração Estéreo\n",
    "\n",
    "Na calibração estéreo, a ideia é encontrar a pose de uma câmera com relação a outra, aproveitando-se dos parâmetros intrísecos que já foram estimados na calibração monocular (mas eles também podem ser refinados se necessário)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca45029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verts2d(path, nverts, criteria):\n",
    "    pathnames = [str(p) for p in sorted(path.glob('*.png'))]\n",
    "    imgs = [cv2.cvtColor(cv2.imread(p)[:,:,0], cv2.COLOR_BAYER_GB2GRAY) for p in pathnames]\n",
    "    verts2d = [cv2.findChessboardCorners(img, nverts) for img in imgs]\n",
    "    return [cv2.cornerSubPix(img, v, (7,7), (-1,-1), criteria) if r else None for img, (r, v) in zip(imgs, verts2d)]\n",
    "\n",
    "tamanho_imagem = cv2.imread(str(imgs_esq_path/'00.png')).shape[1::-1]\n",
    "nverts = (10, 7)\n",
    "\n",
    "l_quadrado = 0.1 # metros\n",
    "verts3d = np.zeros((nverts[0] * nverts[1], 3), dtype=np.float32)\n",
    "i = 0\n",
    "for y in range(nverts[1]):\n",
    "    for x in range(nverts[0]):\n",
    "        verts3d[i] = (x * l_quadrado, y * l_quadrado, 0)\n",
    "        i += 1\n",
    "\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 5e-4)\n",
    "verts2d_esq = get_verts2d(imgs_esq_path, nverts, criteria)\n",
    "verts2d_dir = get_verts2d(imgs_dir_path, nverts, criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b206b",
   "metadata": {},
   "source": [
    "Faça a calibração estéreo de forma a achar a rotação e translação de uma câmera com relação a outra. Use a função `cv2.stereoCalibrateExtended`. Tome cuidado que `objectPoints` é uma lista de pontos 3d de cada imagem (`verts3d` que é constante). Outro cuidado é para filtrar os `verts2d` para ficar apenas com os pares que existem em ambas as câmeras.\n",
    "A calibração estéreo também conseguiria estimar os parâmetros intrísecos, mas é mais estável fazer isso em uma calibração mono (a quantidade de parâmetros para cada estimação diminui e fica mais comportado). A flag padrão já é cv2.CALIB_FIX_INTRINSIC então não precisa se preocupar. Para os vetores R e T de argumentos pode passar `None` ou uma matriz zerada.  (1,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd0295",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17d8a610623ba9361ed0e69671ac0e86",
     "grade": false,
     "grade_id": "calib_stereo",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calibra_estereo(verts3d, verts2d_esq, verts2d_dir, cam_esq, cam_dir, tamanho_imagem):\n",
    "    \"\"\"\n",
    "    Executa a calibração estéreo de um par de câmeras, sem otimizar os parâmetros intrísecos\n",
    "    :param verts3d: As coordenadas dos pontos 3d dos marcadores em seu referencial próprio\n",
    "    :param verts2d_esq: Os pontos 2d dos marcadores na imagem da câmera esquerda\n",
    "    :param verts2d_dir: Os pontos 2d dos marcadores na imagem da câmera direita\n",
    "    :param cam_esq: Tupla com a matriz intríseca e coeficientes de distorção da câmera esquerda\n",
    "    :param cam_dir: Tupla com a matriz intríseca e coeficientes de distorção da câmera direita\n",
    "    :param tamanho_imagem: Tupla com o tamanho da imagem em pixels em largura por altura\n",
    "    Retorna o erro de reprojeção médio de toda a calibração, a matriz de rotação e vetor de translação que levam \n",
    "    pontos 3d do referencial da câmera da esquerda para a direita, a matriz essencial e a fundamental dessa \n",
    "    transformação, e o erro de reprojeção médio para cada imagem.\n",
    "    \"\"\"\n",
    "    me, de = cam_esq\n",
    "    md, dd = cam_dir\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return erro_medio, matriz_rotacao, translacao, matriz_essencial, matriz_fundamental, erro_para_cada_imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ed381",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83d4416734ddce3ccbc265e652e8f685",
     "grade": true,
     "grade_id": "testa_calib_stereo",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cam_esq = (np.array([[1.27677228e+03, 0.00000000e+00, 9.46863329e+02],\n",
    "                     [0.00000000e+00, 1.28093814e+03, 5.26800967e+02],\n",
    "                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]], dtype=np.float32),\n",
    "    np.array([ 0.00463763,  0.00506855, -0.00252541, -0.00054011,  0.        ], dtype=np.float32))\n",
    "cam_dir = (np.array([[1.28316167e+03, 0.00000000e+00, 9.83705999e+02],\n",
    "                     [0.00000000e+00, 1.28959033e+03, 4.74866554e+02],\n",
    "                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]),\n",
    "    np.array([ 0.00438509,  0.01102175, -0.00740251,  0.00029485,  0.        ], dtype=np.float32))\n",
    "err, rmat, T, E, F, erro_para_cada_imagem = calibra_estereo(verts3d, verts2d_esq[:-6], \n",
    "                                                            verts2d_dir[:-6], cam_esq, cam_dir, tamanho_imagem)\n",
    "assert err < 0.7\n",
    "assert np.mean(erro_para_cada_imagem) < 0.7\n",
    "assert erro_para_cada_imagem.shape == (28, 2)\n",
    "assert np.linalg.norm(rmat - np.array([[ 0.9330993 , -0.00167611,  0.35961491],\n",
    "                                       [-0.00388421,  0.99988383,  0.01473875],\n",
    "                                       [-0.35959784, -0.01514954,  0.9329844 ]])) < 1e-5\n",
    "assert np.linalg.norm(T - np.array([[-9.54630632e-01],[ 3.48152961e-04],[ 1.50440555e-01]])) < 1e-5\n",
    "assert np.linalg.norm(E - np.array([[ 4.59148223e-04, -1.50428353e-01, -1.89248456e-03],\n",
    "                                    [-2.02907133e-01, -1.47143694e-02,  9.44756152e-01],\n",
    "                                    [ 3.38312816e-03, -9.54519153e-01, -1.41952640e-02]])) < 1e-5\n",
    "assert np.linalg.norm(F - np.array([[-3.71746733e-09,  1.21397368e-06, -6.16439367e-04],\n",
    "                                    [ 1.63463644e-06,  1.18154655e-07, -1.13275774e-02],\n",
    "                                    [-8.07724752e-04,  8.63399180e-03,  1.00000000e+00]])) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34232946",
   "metadata": {},
   "source": [
    "Veja que um dos valores de retorno do OpenCV são as matrizes Essencial (E) e Fundamental (F). Explique o que são essas matrizes e como elas relacionam os pontos observados por uma câmera para outra. Explique também como elas se relacionam com as matrizes intrínsecas, matrizes de rotação e vetores de translação entre as câmeras. Escreva sua resposta na célula abaixo, pode usar latex se quiser. (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5b1cc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f2a180676f21f69c2f45013d3800605",
     "grade": true,
     "grade_id": "discute_estereo",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**QUESTÃO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (pode apagar comentário, mas não apague esta célula para não perder o ID)\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6de5d",
   "metadata": {},
   "source": [
    "## Retificação Estéreo\n",
    "\n",
    "Encontrar homografias em cada uma das câmeras que torna as linhas epipolares paralelas (epipolos no infinito).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28903ba6",
   "metadata": {},
   "source": [
    "Use a função `cv2.stereoRectify` para encontrar a matriz de retificação de cada uma das câmeras. Além dessa homografia (que é justamente a rotação do centro de projeção), ela também atualiza a matriz de projeção (que é a multiplicação da matriz intrínseca pela extrínseca `K[RT]`) (1,5 pontos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a6d54",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f56f7001bd136e102f81f40b2b5125ba",
     "grade": false,
     "grade_id": "retifica",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def retifica_estereo(cam_esq, cam_dir, tamanho_imagem, matriz_rotacao_entre_cameras, translacao_entre_cameras):\n",
    "    \"\"\"\n",
    "    Faz a retificação estéreo a fim de transformar as linhas epipolares das projeções em linhas horizontais\n",
    "    :param cam_esq: Tupla com a matriz intríseca e coeficientes de distorção da câmera esquerda\n",
    "    :param cam_dir: Tupla com a matriz intríseca e coeficientes de distorção da câmera direita\n",
    "    :param tamanho_imagem: Tupla com o tamanho da imagem em pixels em largura por altura\n",
    "    :param matriz_rotacao_entre_cameras: A matriz de rotação da câmera esquerda medida no referencial da direita\n",
    "    :param translação_entre_cameras: A posição da câmera esquerda medida no referencial da câmera direita\n",
    "    Retorna as matrizes de retificação esquerda e direita, as matrizes de projeção esquerda e direita, a matriz\n",
    "    de mapeamento de disparidade para profundidade.\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return retifica_mat_esq, retifica_mat_dir, projeta_mat_esq, projeta_mat_dir, Q\n",
    "\n",
    "def aplica_mapeamento(imagem, cam_param, retifica_mat, projeta_mat, tamanho_imagem):\n",
    "    mapeamentos = cv2.initUndistortRectifyMap(*cam_param, retifica_mat, projeta_mat, tamanho_imagem, cv2.CV_32FC1)\n",
    "    return cv2.remap(imagem, *mapeamentos, cv2.INTER_LANCZOS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc96eb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40e808577af924ff9fc932b4817c8efe",
     "grade": true,
     "grade_id": "testa_retifica",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "R1, R2, P1, P2, Q = retifica_estereo(cam_esq, cam_dir, tamanho_imagem, rmat, T)\n",
    "assert np.linalg.norm(R1 - np.array([[ 9.77703755e-01,  3.42425370e-04,  2.09988690e-01],\n",
    "                                     [-1.94226256e-03,  9.99970641e-01,  7.41250433e-03],\n",
    "                                     [-2.09979987e-01, -7.65508649e-03,  9.77675715e-01]])) < 1e-5\n",
    "assert np.linalg.norm(P1 - np.array([[1.28526422e+03, 0.00000000e+00, 9.02986206e+02, 0.00000000e+00],\n",
    "                                     [0.00000000e+00, 1.28526422e+03, 4.93165016e+02, 0.00000000e+00],\n",
    "                                     [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00]])) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e694716",
   "metadata": {},
   "source": [
    "Observe como as imagens ficam após a retificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496499f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_esq = cv2.cvtColor(cv2.imread(str(imgs_esq_path/'23.png'), cv2.IMREAD_ANYCOLOR), cv2.COLOR_BAYER_GB2RGB)\n",
    "img_dir = cv2.cvtColor(cv2.imread(str(imgs_dir_path/'23.png'), cv2.IMREAD_ANYCOLOR), cv2.COLOR_BAYER_GB2RGB)\n",
    "\n",
    "img_esq_ret_full = aplica_mapeamento(img_esq, cam_esq, R1, P1, tamanho_imagem)\n",
    "img_dir_ret_full = aplica_mapeamento(img_dir, cam_dir, R2, P2, tamanho_imagem)\n",
    "img_esq_ret_full = np.clip(img_esq_ret_full.astype(np.float32)*1.3, 0, 255).astype(np.uint8)\n",
    "img_dir_ret_full = np.clip(img_dir_ret_full.astype(np.float32)*1.3, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(img_esq_ret_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.fromarray(img_dir_ret_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6e50b",
   "metadata": {},
   "source": [
    "## Disparidade\n",
    "\n",
    "Vamos agora usar um correlacionador que faz uso das imagens retificadas, isto é, ele supoem que o vínculo epipolar acontece apenas nas linhas horizontais (epipolares paralelas e horizontais).\n",
    "\n",
    "Sinta-se livre para mudar os parâmetros abaixo, sobretudo os valores de `blockSize` que define o tamanho da janela de correlação e o `speckleWindowSize` e `speckleRange` que filtram pontos espúrios. Perceba que como as câmeras estão afastadas de 1 metro (e também estava inclindas convergentemente), a disparidade chega a ser da ordem de até 500 pixels para o ponto mais próximo, o que está evidenciado pelo `numDisparities=16*32 ` = 512 (160 depois do resize). Apenas para pontos no infinito teríamos `minDisparity=0`.\n",
    "\n",
    "Observe que devido à falta de textura na imagem, sobretudo no chão e nas paredes, a maior parte dos pontos ficou sem determinação de profundidade, porque não encontrou o seu par (não deu match). Como as disparidades também são grandes ele fica encontrado muitas correlações espúrias.\n",
    "\n",
    "Veja também que precisamos diminuir o tamanho da imagem para deixar o resultado com menos ruídos, experimente outros valores de s (maiores ou menores) que diminuem ou aumenta a escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.22\n",
    "img_esq_ret = cv2.resize(img_esq_ret_full, None, None, s, s, cv2.INTER_AREA)\n",
    "img_dir_ret = cv2.resize(img_dir_ret_full, None, None, s, s, cv2.INTER_AREA)\n",
    "print(img_esq_ret.shape)\n",
    "PIL.Image.fromarray(img_esq_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacionador = cv2.StereoSGBM_create(minDisparity=0, numDisparities=int(16*32*s), blockSize=11, \n",
    "                  P1=8*3*11*11, P2=32*3*11*11, speckleWindowSize = 50, speckleRange = 2, disp12MaxDiff=20)\n",
    "corr_esq = correlacionador.compute(img_esq_ret, img_dir_ret)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_esq, cmap='gray', vmin=0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de35952",
   "metadata": {},
   "source": [
    "Podemos reaplicar o correlacionador com as imagens trocadas e depois utilizar um filtro entre os resultados antípodos:\n",
    "\n",
    "Observe que as cores ficam invertidas, na realidade é porque a disparidade que antes estava positiva, ficou negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44130af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacionador_inverso = cv2.ximgproc.createRightMatcher(correlacionador)\n",
    "corr_dir = correlacionador_inverso.compute(img_dir_ret, img_esq_ret)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_dir, cmap='gray', vmax=0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384172c",
   "metadata": {},
   "source": [
    "E aplicando o filtro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro = cv2.ximgproc.createDisparityWLSFilter(matcher_left=correlacionador)\n",
    "filtro.setLambda(1300)\n",
    "filtro.setSigmaColor(1.9)\n",
    "resultado_filtrado = filtro.filter(np.int16(corr_esq), img_esq_ret, None, np.int16(corr_dir))\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(resultado_filtrado, cmap='gray',vmin=-16*16*2)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35e2986",
   "metadata": {},
   "source": [
    "Para uma imagem com mais textura e com disparidades menores (da ordem de apenas 16 pixels): \n",
    "\n",
    "(OBS ela já estão retificadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "iesq = cv2.imread(str(tsukuba_path/\"tsukuba_l.png\"), cv2.IMREAD_ANYCOLOR)\n",
    "idir = cv2.imread(str(tsukuba_path/\"tsukuba_r.png\"), cv2.IMREAD_ANYCOLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0643ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacionador = cv2.StereoSGBM_create(minDisparity=-16, numDisparities=32, blockSize=9, \n",
    "                  P1=4*3*9*9, P2=16*3*9*9, speckleWindowSize = 50, speckleRange = 1, disp12MaxDiff=20)\n",
    "\n",
    "corr_esq = correlacionador.compute(iesq, idir)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.axis(False)\n",
    "plt.imshow(corr_esq, cmap='gray', vmin=0)\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
